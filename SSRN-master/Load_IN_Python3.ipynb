{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished importing modules in *Load_IN.py* at Thu May  2 19:16:16 2019\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv3D, MaxPooling3D, ZeroPadding3D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, Input\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.optimizers import Adam, SGD, Adadelta, RMSprop, Nadam\n",
    "import keras.callbacks as kcallbacks\n",
    "from keras.regularizers import l2\n",
    "import time\n",
    "import collections\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "from Utils import zeroPadding, normalization, doPCA, modelStatsRecord, averageAccuracy, ssrn_SS_IN\n",
    "\n",
    "print('Finished importing modules in *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished *indexToAssignment* module in *Load_IN.py* at Thu May  2 19:16:17 2019\n"
     ]
    }
   ],
   "source": [
    "def indexToAssignment(index_, Row, Col, pad_length):\n",
    "    new_assign = {}\n",
    "    for counter, value in enumerate(index_):\n",
    "        assign_0 = value // Col + pad_length\n",
    "        assign_1 = value % Col + pad_length\n",
    "        new_assign[counter] = [assign_0, assign_1]\n",
    "    return new_assign\n",
    "print('Finished *indexToAssignment* module in *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished *assignmentToIndex* module in *Load_IN.py* at Thu May  2 19:16:18 2019\n"
     ]
    }
   ],
   "source": [
    "def assignmentToIndex(assign_0, assign_1, Row, Col):\n",
    "    new_index = assign_0 * Col + assign_1\n",
    "    return new_index\n",
    "print('Finished *assignmentToIndex* module in *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished *selectNeighboringPatch* module in *Load_IN.py* at Thu May  2 19:16:19 2019\n"
     ]
    }
   ],
   "source": [
    "def selectNeighboringPatch(matrix, pos_row, pos_col, ex_len):\n",
    "    selected_rows = matrix[range(pos_row - ex_len, pos_row + ex_len + 1), :]\n",
    "    selected_patch = selected_rows[:, range(pos_col - ex_len, pos_col + ex_len + 1)]\n",
    "    return selected_patch\n",
    "print('Finished *selectNeighboringPatch* module in *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished *sampling* module in *Load_IN.py* at Thu May  2 19:16:20 2019\n"
     ]
    }
   ],
   "source": [
    "def sampling(proptionVal, groundTruth):  # divide dataset into train and test datasets\n",
    "    labels_loc = {}\n",
    "    train = {}\n",
    "    test = {}\n",
    "    m = max(groundTruth)\n",
    "    for i in range(m):\n",
    "        indices = [j for j, x in enumerate(groundTruth.ravel().tolist()) if x == i + 1]\n",
    "        np.random.shuffle(indices)\n",
    "        labels_loc[i] = indices\n",
    "        nb_val = int(proptionVal * len(indices))\n",
    "        train[i] = indices[:-nb_val]\n",
    "        test[i] = indices[-nb_val:]\n",
    "    # whole_indices = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(m):\n",
    "        #        whole_indices += labels_loc[i]\n",
    "        train_indices += train[i]\n",
    "        test_indices += test[i]\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    return train_indices, test_indices\n",
    "print('Finished *sampling* module in *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished *res4model_ss* module in *Load_IN.py* at Thu May  2 19:16:22 2019\n"
     ]
    }
   ],
   "source": [
    "def res4_model_ss():\n",
    "    model_res4 = ssrn_SS_IN.ResnetBuilder.build_resnet_8((1, img_rows, img_cols, img_channels), nb_classes)\n",
    "\n",
    "    RMS = RMSprop(lr=0.0003)\n",
    "    # Let's train the model using RMSprop\n",
    "    model_res4.compile(loss='categorical_crossentropy', optimizer=RMS, metrics=['accuracy'])\n",
    "\n",
    "    return model_res4\n",
    "print('Finished *res4model_ss* module in *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_IN.shape:\n",
      "(145, 145, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint16 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 Iteration\n",
      "input shape: 200\n",
      "input shape: (None, 7, 7, 97, 24)\n",
      "conv_spc_result shape: (None, 7, 7, 1, 128)\n",
      "conv1 shape: (None, 5, 5, 1, 24)\n",
      "input shape: (None, 5, 5, 1, 24)\n",
      "3D RESNET_SS4 without BN training finished.\n",
      "# 1 Iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\Desktop\\RS\\residualnet_tensorflow_keras\\residualnet_tensorflow_keras\\SSRN-master\\Utils\\averageAccuracy.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n"
     ]
    }
   ],
   "source": [
    "mat_data = sio.loadmat(r'''C:/Users/josep/Desktop/RS/residualnet_tensorflow_keras/residualnet_tensorflow_keras/SSRN-master/datasets/IN/Indian_pines_corrected.mat''')\n",
    "data_IN = mat_data['indian_pines_corrected']\n",
    "mat_gt = sio.loadmat(r'''C:/Users/josep/Desktop/RS/residualnet_tensorflow_keras/residualnet_tensorflow_keras/SSRN-master/datasets/IN/Indian_pines_gt.mat''')\n",
    "gt_IN = mat_gt['indian_pines_gt']\n",
    "print('data_IN.shape:')\n",
    "print (data_IN.shape)\n",
    "\n",
    "# new_gt_IN = set_zeros(gt_IN, [1,4,7,9,13,15,16])\n",
    "new_gt_IN = gt_IN\n",
    "\n",
    "batch_size = 16\n",
    "nb_classes = 16\n",
    "nb_epoch = 200  # 400\n",
    "img_rows, img_cols = 7, 7  # 27, 27\n",
    "patience = 200\n",
    "\n",
    "INPUT_DIMENSION_CONV = 200\n",
    "INPUT_DIMENSION = 200\n",
    "\n",
    "# 20%:10%:70% data for training, validation and testing\n",
    "\n",
    "TOTAL_SIZE = 10249\n",
    "VAL_SIZE = 1025\n",
    "\n",
    "TRAIN_SIZE = 2055\n",
    "TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "VALIDATION_SPLIT = 0.8\n",
    "# TRAIN_NUM = 10\n",
    "# TRAIN_SIZE = TRAIN_NUM * nb_classes\n",
    "# TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE\n",
    "# VAL_SIZE = TRAIN_SIZE\n",
    "\n",
    "\n",
    "img_channels = 200\n",
    "PATCH_LENGTH = 3  # Patch_size (13*2+1)*(13*2+1)\n",
    "\n",
    "data = data_IN.reshape(np.prod(data_IN.shape[:2]), np.prod(data_IN.shape[2:]))\n",
    "gt = new_gt_IN.reshape(np.prod(new_gt_IN.shape[:2]), )\n",
    "\n",
    "data = preprocessing.scale(data)\n",
    "\n",
    "# scaler = preprocessing.MaxAbsScaler()\n",
    "# data = scaler.fit_transform(data)\n",
    "\n",
    "data_ = data.reshape(data_IN.shape[0], data_IN.shape[1], data_IN.shape[2])\n",
    "whole_data = data_\n",
    "padded_data = zeroPadding.zeroPadding_3D(whole_data, PATCH_LENGTH)\n",
    "\n",
    "ITER = 1\n",
    "CATEGORY = 16\n",
    "\n",
    "train_data = np.zeros((TRAIN_SIZE, 2 * PATCH_LENGTH + 1, 2 * PATCH_LENGTH + 1, INPUT_DIMENSION_CONV))\n",
    "test_data = np.zeros((TEST_SIZE, 2 * PATCH_LENGTH + 1, 2 * PATCH_LENGTH + 1, INPUT_DIMENSION_CONV))\n",
    "\n",
    "KAPPA_RES_SS4 = []\n",
    "OA_RES_SS4 = []\n",
    "AA_RES_SS4 = []\n",
    "TRAINING_TIME_RES_SS4 = []\n",
    "TESTING_TIME_RES_SS4 = []\n",
    "ELEMENT_ACC_RES_SS4 = np.zeros((ITER, CATEGORY))\n",
    "\n",
    "# seeds = [1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229]\n",
    "\n",
    "seeds = [1334]\n",
    "\n",
    "for index_iter in range(ITER):\n",
    "    print(\"# %d Iteration\" % (index_iter + 1))\n",
    "\n",
    "    best_weights_RES_path_ss4 = r'''C:/Users/josep/Desktop/RS/residualnet_tensorflow_keras/residualnet_tensorflow_keras/SSRN-master/models/Indian_best_RES_3D_SS4_10_''' + str(\n",
    "        index_iter + 1) + '.hdf5'\n",
    "\n",
    "    np.random.seed(seeds[index_iter])\n",
    "    #    train_indices, test_indices = sampleFixNum.samplingFixedNum(TRAIN_NUM, gt)\n",
    "    train_indices, test_indices = sampling(VALIDATION_SPLIT, gt)\n",
    "\n",
    "    # TRAIN_SIZE = len(train_indices)\n",
    "    # print (TRAIN_SIZE)\n",
    "    #\n",
    "    # TEST_SIZE = TOTAL_SIZE - TRAIN_SIZE - VAL_SIZE\n",
    "    # print (TEST_SIZE)\n",
    "\n",
    "    y_train = gt[train_indices] - 1\n",
    "    y_train = to_categorical(np.asarray(y_train))\n",
    "\n",
    "    y_test = gt[test_indices] - 1\n",
    "    y_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    "    # print (\"Validation data:\")\n",
    "    # collections.Counter(y_test_raw[-VAL_SIZE:])\n",
    "    # print (\"Testing data:\")\n",
    "    # collections.Counter(y_test_raw[:-VAL_SIZE])\n",
    "\n",
    "    train_assign = indexToAssignment(train_indices, whole_data.shape[0], whole_data.shape[1], PATCH_LENGTH)\n",
    "    for i in range(len(train_assign)):\n",
    "        train_data[i] = selectNeighboringPatch(padded_data, train_assign[i][0], train_assign[i][1], PATCH_LENGTH)\n",
    "\n",
    "    test_assign = indexToAssignment(test_indices, whole_data.shape[0], whole_data.shape[1], PATCH_LENGTH)\n",
    "    for i in range(len(test_assign)):\n",
    "        test_data[i] = selectNeighboringPatch(padded_data, test_assign[i][0], test_assign[i][1], PATCH_LENGTH)\n",
    "\n",
    "    x_train = train_data.reshape(train_data.shape[0], train_data.shape[1], train_data.shape[2], INPUT_DIMENSION_CONV)\n",
    "    x_test_all = test_data.reshape(test_data.shape[0], test_data.shape[1], test_data.shape[2], INPUT_DIMENSION_CONV)\n",
    "\n",
    "    x_val = x_test_all[-VAL_SIZE:]\n",
    "    y_val = y_test[-VAL_SIZE:]\n",
    "\n",
    "    x_test = x_test_all[:-VAL_SIZE]\n",
    "    y_test = y_test[:-VAL_SIZE]\n",
    "\n",
    "    # SS Residual Network 4 with BN\n",
    "    model_res4_SS_BN = res4_model_ss()\n",
    "\n",
    "    model_res4_SS_BN.load_weights(best_weights_RES_path_ss4)\n",
    "\n",
    "    pred_test_res4 = model_res4_SS_BN.predict(\n",
    "        x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], x_test.shape[3], 1)).argmax(axis=1)\n",
    "    collections.Counter(pred_test_res4)\n",
    "    gt_test = gt[test_indices] - 1\n",
    "    overall_acc_res4 = metrics.accuracy_score(pred_test_res4, gt_test[:-VAL_SIZE])\n",
    "    confusion_matrix_res4 = metrics.confusion_matrix(pred_test_res4, gt_test[:-VAL_SIZE])\n",
    "    each_acc_res4, average_acc_res4 = averageAccuracy.AA_andEachClassAccuracy(confusion_matrix_res4)\n",
    "    kappa = metrics.cohen_kappa_score(pred_test_res4, gt_test[:-VAL_SIZE])\n",
    "    KAPPA_RES_SS4.append(kappa)\n",
    "    OA_RES_SS4.append(overall_acc_res4)\n",
    "    AA_RES_SS4.append(average_acc_res4)\n",
    "    #TRAINING_TIME_RES_SS4.append(toc6 - tic6)\n",
    "    #TESTING_TIME_RES_SS4.append(toc7 - tic7)\n",
    "    ELEMENT_ACC_RES_SS4[index_iter, :] = each_acc_res4\n",
    "\n",
    "    print(\"3D RESNET_SS4 without BN training finished.\")\n",
    "    print(\"# %d Iteration\" % (index_iter + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished *Load_IN.py* at Thu May  2 19:17:30 2019\n"
     ]
    }
   ],
   "source": [
    "modelStatsRecord.outputStats_assess(KAPPA_RES_SS4, OA_RES_SS4, AA_RES_SS4, ELEMENT_ACC_RES_SS4, CATEGORY,\n",
    "                             r'''C:\\Users\\josep\\Desktop\\RS\\residualnet_tensorflow_keras\\residualnet_tensorflow_keras\\SSRN-master\\records\\IN_test_SS_10.txt''',\n",
    "                             r'''C:\\Users\\josep\\Desktop\\RS\\residualnet_tensorflow_keras\\residualnet_tensorflow_keras\\SSRN-master\\records\\IN_test_SS_element_10.txt''')\n",
    "    \n",
    "    \n",
    "print('Finished *Load_IN.py* at ' + time.ctime(time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
